{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451247b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# For Jupyter Notebooks / IPython\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime, timezone\n",
    "from ib_insync import IB, Stock, util\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Config --------\n",
    "IB_HOST = '127.0.0.1'\n",
    "IB_PORT = 7496       # 7497 = paper, 7496 = live\n",
    "CLIENT_ID = 102      # Different from your streamer\n",
    "\n",
    "# For Jupyter notebooks, use util.startLoop()\n",
    "util.startLoop()\n",
    "\n",
    "async def get_historical_data():\n",
    "    ib = IB()\n",
    "    await ib.connectAsync(IB_HOST, IB_PORT, clientId=CLIENT_ID)\n",
    "    print(\"Connected to IBKR\")\n",
    "    \n",
    "    # Qualify QQQ\n",
    "    [qqq] = await ib.qualifyContractsAsync(Stock('QQQ', 'ARCA', 'USD'))\n",
    "    \n",
    "    # Get 1-minute bars for last 2 days\n",
    "    bars_1m = await ib.reqHistoricalDataAsync(\n",
    "        qqq,\n",
    "        endDateTime='',  # Empty string = now\n",
    "        durationStr='2 D',  # 2 days of data\n",
    "        barSizeSetting='1 min',\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False,  # Include pre/post market\n",
    "        formatDate=1\n",
    "    )\n",
    "    \n",
    "    # Get 5-minute bars for last week\n",
    "    bars_5m = await ib.reqHistoricalDataAsync(\n",
    "        qqq,\n",
    "        endDateTime='',\n",
    "        durationStr='1 W',  # 1 week of data\n",
    "        barSizeSetting='5 mins',\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False,\n",
    "        formatDate=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n1-MINUTE BARS (last 10):\")\n",
    "    print(\"─\" * 60)\n",
    "    for bar in bars_1m[-10:]:  # Last 10 bars\n",
    "        print(f\"{bar.date} | O:{bar.open:7.2f} H:{bar.high:7.2f} \"\n",
    "              f\"L:{bar.low:7.2f} C:{bar.close:7.2f} V:{bar.volume:8.0f}\")\n",
    "    \n",
    "    print(f\"\\n5-MINUTE BARS (last 10):\")\n",
    "    print(\"─\" * 60)\n",
    "    for bar in bars_5m[-10:]:\n",
    "        print(f\"{bar.date} | O:{bar.open:7.2f} H:{bar.high:7.2f} \"\n",
    "              f\"L:{bar.low:7.2f} C:{bar.close:7.2f} V:{bar.volume:8.0f}\")\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df_1m = pd.DataFrame([{\n",
    "        'date': bar.date,\n",
    "        'open': bar.open,\n",
    "        'high': bar.high,\n",
    "        'low': bar.low,\n",
    "        'close': bar.close,\n",
    "        'volume': bar.volume\n",
    "    } for bar in bars_1m])\n",
    "    \n",
    "    df_5m = pd.DataFrame([{\n",
    "        'date': bar.date,\n",
    "        'open': bar.open,\n",
    "        'high': bar.high,\n",
    "        'low': bar.low,\n",
    "        'close': bar.close,\n",
    "        'volume': bar.volume\n",
    "    } for bar in bars_5m])\n",
    "    \n",
    "    # Save to CSV if needed\n",
    "    df_1m.to_csv('qqq_1m.csv', index=False)\n",
    "    df_5m.to_csv('qqq_5m.csv', index=False)\n",
    "    print(f\"\\nSaved {len(df_1m)} 1m bars to qqq_1m.csv\")\n",
    "    print(f\"Saved {len(df_5m)} 5m bars to qqq_5m.csv\")\n",
    "    \n",
    "    ib.disconnect()\n",
    "    return df_1m, df_5m\n",
    "\n",
    "# SYNCHRONOUS VERSION (easier for notebooks)\n",
    "def get_historical_sync():\n",
    "    ib = IB()\n",
    "    ib.connect(IB_HOST, IB_PORT, clientId=CLIENT_ID)\n",
    "    print(\"Connected to IBKR\")\n",
    "    \n",
    "    # Qualify QQQ\n",
    "    qqq = Stock('QQQ', 'ARCA', 'USD')\n",
    "    ib.qualifyContracts(qqq)\n",
    "    \n",
    "    # Get 1-minute bars\n",
    "    bars_1m = ib.reqHistoricalData(\n",
    "        qqq,\n",
    "        endDateTime='',\n",
    "        durationStr='2 D',\n",
    "        barSizeSetting='1 min',\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False,\n",
    "        formatDate=1  # Add this to get datetime objects\n",
    "    )\n",
    "    \n",
    "    # Get 5-minute bars\n",
    "    bars_5m = ib.reqHistoricalData(\n",
    "        qqq,\n",
    "        endDateTime='',\n",
    "        durationStr='1 W',\n",
    "        barSizeSetting='5 mins',\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False,\n",
    "        formatDate=1  # Add this to get datetime objects\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame - handle timezone issues\n",
    "    df_1m = pd.DataFrame([{\n",
    "        'date': pd.Timestamp(bar.date).tz_localize(None),  # Remove timezone\n",
    "        'open': bar.open,\n",
    "        'high': bar.high,\n",
    "        'low': bar.low,\n",
    "        'close': bar.close,\n",
    "        'volume': bar.volume\n",
    "    } for bar in bars_1m])\n",
    "    \n",
    "    df_5m = pd.DataFrame([{\n",
    "        'date': pd.Timestamp(bar.date).tz_localize(None),  # Remove timezone\n",
    "        'open': bar.open,\n",
    "        'high': bar.high,\n",
    "        'low': bar.low,\n",
    "        'close': bar.close,\n",
    "        'volume': bar.volume\n",
    "    } for bar in bars_5m])\n",
    "    \n",
    "    print(f\"Got {len(df_1m)} 1-minute bars\")\n",
    "    print(f\"Got {len(df_5m)} 5-minute bars\")\n",
    "    \n",
    "    ib.disconnect()\n",
    "    return df_1m, df_5m\n",
    "\n",
    "# FOR JUPYTER - Use await directly or the sync version:\n",
    "# Option 1: Await the async function\n",
    "# df_1m, df_5m = await get_historical_data()\n",
    "\n",
    "# Option 2: Use the sync version (easier!)\n",
    "# df_1m, df_5m = get_historical_sync()\n",
    "\n",
    "# Quick function to just get recent data - FIXED VERSION\n",
    "def get_quick_data(symbol='QQQ', days=1, bar_size='1 min'):\n",
    "    ib = IB()\n",
    "    ib.connect(IB_HOST, IB_PORT, clientId=CLIENT_ID)\n",
    "    \n",
    "    stock = Stock(symbol, 'ARCA', 'USD')\n",
    "    ib.qualifyContracts(stock)\n",
    "    \n",
    "    bars = ib.reqHistoricalData(\n",
    "        stock,\n",
    "        endDateTime='',\n",
    "        durationStr=f'{days} D',\n",
    "        barSizeSetting=bar_size,\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False,\n",
    "        formatDate=2  # Use string format to avoid timezone issues\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame with strings first, then convert\n",
    "    data = []\n",
    "    for bar in bars:\n",
    "        data.append({\n",
    "            'date': str(bar.date),  # Keep as string\n",
    "            'open': float(bar.open),\n",
    "            'high': float(bar.high),\n",
    "            'low': float(bar.low),\n",
    "            'close': float(bar.close),\n",
    "            'volume': int(bar.volume),\n",
    "            'average': float(bar.average),\n",
    "            'barCount': int(bar.barCount)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Optionally convert date to datetime without timezone\n",
    "    # df['date'] = pd.to_datetime(df['date'], format='%Y%m%d %H:%M:%S')\n",
    "    \n",
    "    ib.disconnect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Even simpler version - no pandas datetime at all\n",
    "def get_data_simple(symbol='QQQ', days=1, bar_size='1 min'):\n",
    "    ib = IB()\n",
    "    ib.connect(IB_HOST, IB_PORT, clientId=CLIENT_ID)\n",
    "    \n",
    "    stock = Stock(symbol, 'ARCA', 'USD')\n",
    "    ib.qualifyContracts(stock)\n",
    "    \n",
    "    bars = ib.reqHistoricalData(\n",
    "        stock,\n",
    "        endDateTime='',\n",
    "        durationStr=f'{days} D',\n",
    "        barSizeSetting=bar_size,\n",
    "        whatToShow='TRADES',\n",
    "        useRTH=False\n",
    "    )\n",
    "    \n",
    "    # Just return the raw bars - no DataFrame conversion\n",
    "    ib.disconnect()\n",
    "    \n",
    "    # Access data like: bars[0].open, bars[0].close, etc.\n",
    "    return bars\n",
    "\n",
    "# Example usage in notebook cells:\n",
    "# Cell 1:\n",
    "# df = get_quick_data('QQQ', days=2, bar_size='5 mins')\n",
    "# df.tail(20)\n",
    "\n",
    "# Cell 2: Plot the data\n",
    "# import matplotlib.pyplot as plt\n",
    "# df['close'].plot(figsize=(12,6))\n",
    "# plt.title('QQQ 5-minute bars')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92784172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SMART QQQ DATA LOADING\n",
      "==================================================\n",
      "📂 Loading CSV data only...\n",
      "📂 Loading 1m data from qqq_1m_backfill.csv\n",
      "   ✅ Loaded 66,059 bars (2025-05-16 08:00:00 to 2025-08-25 23:59:00)\n",
      "📂 Loading 5m data from qqq_5m_backfill.csv\n",
      "   ✅ Loaded 13,596 bars (2025-05-14 08:00:00 to 2025-08-25 23:55:00)\n",
      "📂 Loading 4h data from qqq_4h_backfill.csv\n",
      "   ✅ Loaded 348 bars (2025-04-22 08:00:00 to 2025-08-25 20:00:00)\n",
      "\n",
      "✅ DATA LOADING COMPLETE\n",
      "   Ready for ORB analysis with 4H confluence!\n",
      "   1m: 66,059 bars ✅\n",
      "   5m: 13,596 bars ✅\n",
      "   4h: 348 bars ✅\n",
      "\n",
      "🎯 4H Confluence Data Check:\n",
      "   4H bars available: 348\n",
      "   Sufficient for RSI (14): ✅\n",
      "   Sufficient for MACD (26): ✅\n"
     ]
    }
   ],
   "source": [
    "# 🎯 ENHANCED DATA LOADING SYSTEM - No more timeouts!\n",
    "# Uses smart fallback: Live IBKR → CSV Backfill → Error handling\n",
    "\n",
    "# Import the enhanced data loader\n",
    "import sys\n",
    "sys.path.append('/Users/mmirandi/Desktop/Finance')\n",
    "from qqq_data_loader import get_qqq_data, quick_csv_load, quick_live_test\n",
    "\n",
    "print(\"🔄 SMART QQQ DATA LOADING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Option 1: Smart loading (tries live first, falls back to CSV)\n",
    "# df_1m, df_5m, df_4h = get_qqq_data(prefer_live=True, live_days=14)\n",
    "\n",
    "# Option 2: CSV only (fastest, most reliable)\n",
    "df_1m, df_5m, df_4h = quick_csv_load()\n",
    "\n",
    "# Option 3: Force live data attempt\n",
    "# df_1m, df_5m, df_4h = get_qqq_data(prefer_live=True, live_days=2)  # Smaller chunks\n",
    "\n",
    "print(f\"\\n✅ DATA LOADING COMPLETE\")\n",
    "print(f\"   Ready for ORB analysis with 4H confluence!\")\n",
    "\n",
    "# Quick data validation\n",
    "for name, df in [('1m', df_1m), ('5m', df_5m), ('4h', df_4h)]:\n",
    "    if not df.empty:\n",
    "        print(f\"   {name}: {len(df):,} bars ✅\")\n",
    "    else:\n",
    "        print(f\"   {name}: No data ❌\")\n",
    "        \n",
    "# Set up data for ORB system\n",
    "if not df_4h.empty:\n",
    "    print(f\"\\n🎯 4H Confluence Data Check:\")\n",
    "    print(f\"   4H bars available: {len(df_4h)}\")\n",
    "    print(f\"   Sufficient for RSI (14): {'✅' if len(df_4h) >= 20 else '❌'}\")\n",
    "    print(f\"   Sufficient for MACD (26): {'✅' if len(df_4h) >= 35 else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbb8ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea791605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1m.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "935296d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VALIDATING 4H DATA FOR ORB CONFLUENCE\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3080\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:101\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:4554\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:4562\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert date strings to datetime for all dataframes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df_name, df \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1m\u001b[39m\u001b[38;5;124m'\u001b[39m, df_1m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m'\u001b[39m, df_5m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4h\u001b[39m\u001b[38;5;124m'\u001b[39m, df_4h)]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Converted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dates to datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:3024\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3024\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3026\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3082\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3082\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, np\u001b[38;5;241m.\u001b[39masarray(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# 🧪 VALIDATION: Test 4H data quality and prepare for ORB system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"🔍 VALIDATING 4H DATA FOR ORB CONFLUENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert date strings to datetime for all dataframes\n",
    "for df_name, df in [('1m', df_1m), ('5m', df_5m), ('4h', df_4h)]:\n",
    "    if df['date'].dtype == 'object':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        print(f\"✅ Converted {df_name} dates to datetime\")\n",
    "\n",
    "# Check 4H data has sufficient history for indicators\n",
    "print(f\"\\n📈 4H Data Analysis:\")\n",
    "print(f\"   Total 4H bars: {len(df_4h)}\")\n",
    "print(f\"   Date range: {df_4h['date'].min()} to {df_4h['date'].max()}\")\n",
    "print(f\"   Days covered: {(df_4h['date'].max() - df_4h['date'].min()).days}\")\n",
    "\n",
    "# Quick indicator viability check\n",
    "print(f\"   RSI viability: {'✅ YES' if len(df_4h) >= 20 else '❌ NO'} (need 20+ bars, have {len(df_4h)})\")\n",
    "print(f\"   MACD viability: {'✅ YES' if len(df_4h) >= 35 else '❌ NO'} (need 35+ bars, have {len(df_4h)})\")\n",
    "\n",
    "# Preview 4H OHLCV for manual inspection  \n",
    "print(f\"\\n🔍 Recent 4H Bars:\")\n",
    "recent_4h = df_4h.tail(10)[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "print(recent_4h.to_string(index=False))\n",
    "\n",
    "# Check for any data issues\n",
    "print(f\"\\n🚨 Data Quality Issues:\")\n",
    "null_check = df_4h.isnull().sum().sum()\n",
    "print(f\"   Null values: {null_check} {'❌ ISSUE' if null_check > 0 else '✅ CLEAN'}\")\n",
    "\n",
    "zero_volume = (df_4h['volume'] == 0).sum()\n",
    "print(f\"   Zero volume bars: {zero_volume} {'❌ ISSUE' if zero_volume > len(df_4h)*0.1 else '✅ ACCEPTABLE'}\")\n",
    "\n",
    "# Test basic calculation compatibility\n",
    "print(f\"\\n🧮 Basic Calculations Test:\")\n",
    "try:\n",
    "    df_test = df_4h.copy()\n",
    "    df_test['sma_5'] = df_test['close'].rolling(5).mean()\n",
    "    df_test['price_change'] = df_test['close'].pct_change()\n",
    "    print(\"   ✅ Calculations working correctly\")\n",
    "    print(f\"   Recent SMA5: {df_test['sma_5'].iloc[-1]:.2f}\")\n",
    "    print(f\"   Recent change: {df_test['price_change'].iloc[-1]*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Calculation error: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 READY FOR ORB SYSTEM INTEGRATION\")\n",
    "print(f\"   All timeframes available: 1m ✅, 5m ✅, 4h ✅\")\n",
    "print(f\"   Data quality: {'✅ PASSED' if null_check == 0 else '⚠️ REVIEW NEEDED'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
